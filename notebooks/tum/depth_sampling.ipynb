{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7369070e-907f-4a65-ae0f-9ba4d9ab33dd",
   "metadata": {},
   "source": [
    "# Test depth sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89662a0f-f66c-4a58-945b-044e8e6a3972",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = None\n",
    "for x in data_module.train_dataloader():\n",
    "    batch = x\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c621f75f-a6e1-4869-b852-71af070a09be",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index = 0\n",
    "y = 304\n",
    "x = 300\n",
    "dataset_index = image_index * 480 * 640 + y * 640 + x\n",
    "element = data_module._dataset[dataset_index]\n",
    "pixel = torch.tensor(element['pixel']).cuda()[None]\n",
    "camera_position = torch.tensor(element['camera_position']).cuda()[None]\n",
    "truth_depth = torch.tensor(element['depth']).cuda()[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c98faf-a553-49b5-a4c5-48686a97e55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pixel = batch['pixel'].cuda()\n",
    "# camera_position = batch['camera_position'].cuda()\n",
    "# truth_depth = batch['depth'].cuda()\n",
    "course_sampled_depths = model.stratified_sample_depths(\n",
    "    pixel.shape[0],\n",
    "    pixel.device,\n",
    "    model.hparams.course_sample_bins,\n",
    "    False)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1001ff1c-a272-46f2-aac7-19e5f9e5e5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=200)\n",
    "plotted_depths = course_sampled_depths[:, 0].detach().cpu().numpy()\n",
    "plt.scatter(np.arange(plotted_depths.shape[0]), plotted_depths, s=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e215d596-5ed8-4135-944e-85c274276449",
   "metadata": {},
   "outputs": [],
   "source": [
    "course_color, course_depths, course_weights, course_depth_variance = model.reconstruct_color_and_depths(\n",
    "    course_sampled_depths,\n",
    "    pixel,\n",
    "    camera_position,\n",
    "    model._mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1938186c-73f4-49d6-b3bc-d67438f5bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_count = course_sampled_depths.shape[0]\n",
    "sampled_depths = torch.sort(course_sampled_depths, dim=0).values\n",
    "sampled_depths = course_sampled_depths.reshape(-1)\n",
    "pixels1 = model.repeat_tensor(pixel, bins_count)\n",
    "camera_positions1 = model.repeat_tensor(camera_position, bins_count)\n",
    "back_projected_points = back_project_pixel(pixels1, sampled_depths, camera_positions1,\n",
    "                                           model._inverted_camera_matrix)\n",
    "encodings = model._positional_encoding(back_projected_points)\n",
    "prediction = model._mlp(encodings)\n",
    "\n",
    "colors = prediction[:, :3].reshape(bins_count, -1, 3)\n",
    "density = prediction[:, 3].reshape(bins_count, -1)\n",
    "\n",
    "logsumexp_density = torch.logsumexp(torch.cat([torch.zeros_like(density)[None], density[None]], dim=0), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b7e07c-e93c-4dfd-977c-dc4eec4b7fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=200)\n",
    "index = 0\n",
    "plotted_depths = course_sampled_depths[:, index].detach().cpu().numpy()\n",
    "plotted_depths = np.append(plotted_depths, 0.4)\n",
    "plotted_weigths = course_weights[:, index].detach().cpu().numpy()\n",
    "plotted_truth_depth = truth_depth[index].detach().cpu().numpy()\n",
    "plotted_density = logsumexp_density[:, index].detach().cpu().numpy()\n",
    "mean_depth = np.sum(plotted_depths * plotted_weigths)\n",
    "plt.plot(plotted_depths, plotted_weigths)\n",
    "plt.plot([plotted_truth_depth, plotted_truth_depth], [0, np.max(plotted_weigths)])\n",
    "plt.plot([mean_depth, mean_depth], [0, np.max(plotted_weigths)])\n",
    "plt.plot(plotted_depths[:-1], plotted_density / 50.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1f505e-7756-4cca-92ce-d0066d670d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_sampled_depths = model.hierarchical_sample_depths(\n",
    "    course_weights,\n",
    "    pixel.shape[0],\n",
    "    pixel.device,\n",
    "    50,\n",
    "    course_sampled_depths,\n",
    "    False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a69ef3d-c9e7-41b4-92fa-70c8f793104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=200)\n",
    "index = 0\n",
    "plotted_course_depths = course_sampled_depths[:, index].detach().cpu().numpy()\n",
    "plotted_course_depths = np.append(plotted_course_depths, 0.4)\n",
    "plotted_fine_depths = fine_sampled_depths[:, index].detach().cpu().numpy()\n",
    "plotted_weigths = course_weights[:, index].detach().cpu().numpy()\n",
    "plt.plot(plotted_course_depths, plotted_weigths)\n",
    "plt.scatter(plotted_fine_depths, np.zeros_like(plotted_fine_depths), s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559cd4fc-7418-4cc7-a5b7-59676ba99c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    fine_color, fine_depths, fine_weights, fine_depth_variance = model.reconstruct_color_and_depths(\n",
    "        fine_sampled_depths,\n",
    "        pixel,\n",
    "        camera_position,\n",
    "        model._mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43427a9b-033e-4554-ad05-61458b26cc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_count = fine_sampled_depths.shape[0]\n",
    "sampled_depths = torch.sort(fine_sampled_depths, dim=0).values\n",
    "sampled_depths = fine_sampled_depths.reshape(-1)\n",
    "pixels1 = model.repeat_tensor(pixel, bins_count)\n",
    "camera_positions1 = model.repeat_tensor(camera_position, bins_count)\n",
    "back_projected_points = back_project_pixel(pixels1, sampled_depths, camera_positions1,\n",
    "                                           model._inverted_camera_matrix)\n",
    "encodings = model._positional_encoding(back_projected_points)\n",
    "prediction = model._mlp(encodings)\n",
    "\n",
    "colors = prediction[:, :3].reshape(bins_count, -1, 3)\n",
    "density = prediction[:, 3].reshape(bins_count, -1)\n",
    "\n",
    "fine_logsumexp_density = torch.logsumexp(torch.cat([torch.zeros_like(density)[None], density[None]], dim=0), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfe4700-9e55-44c5-b025-a4b8cff3ac0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=200)\n",
    "index = 0\n",
    "plotted_depths = torch.sort(fine_sampled_depths[:, index], dim=0).values.detach().cpu().numpy()\n",
    "depth_deltas =  np.roll(plotted_depths, -1) - plotted_depths\n",
    "plotted_weigths = fine_weights[:, index].detach().cpu().numpy()[:-1]\n",
    "plotted_truth_depth = truth_depth[index].detach().cpu().numpy()\n",
    "mean_depth = np.sum(plotted_depths * plotted_weigths)\n",
    "\n",
    "plt.plot(plotted_depths, plotted_weigths / depth_deltas / 100)\n",
    "\n",
    "plt.plot([plotted_truth_depth, plotted_truth_depth], [0, np.max(plotted_weigths / depth_deltas / 100)])\n",
    "plt.plot([mean_depth, mean_depth], [0, np.max(plotted_weigths / depth_deltas / 100)])\n",
    "\n",
    "plotted_density = fine_logsumexp_density[:, index].detach().cpu().numpy()\n",
    "plt.plot(plotted_depths, plotted_density / 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1d9715-d931-4a1f-b6ca-7080bfa27f09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343f3bc9-1054-4c7d-bac4-742830ea24ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3188a719-8fa4-4956-834d-a608124deb6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5445bf-d470-48c4-beab-f2024ba278e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533f73f1-646e-4cb6-b103-ed8488ad6f43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9292f304-ad30-468d-a702-6874458987f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f04a2c1-3fd5-4362-ac80-c21df4fcc89a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
